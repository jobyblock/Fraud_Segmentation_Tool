{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4384df3c-59c8-48c1-901f-fbb01b48182d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Fraud Segmentation Tool \n",
    "## Date: 10/16/2024\n",
    "## Author: Joby George\n",
    "\n",
    "# Background\n",
    "\n",
    "Fraud Decision Scientsits often have to clean up rules, this can invovle a couple of primary activities, namely:\n",
    "\n",
    "  a. creating new rules from scratch given a new feature or set of features\n",
    "\n",
    " \n",
    "  b. changing the splitting points of features that are currently being used to decrease action rate or increase accuracy\n",
    "\n",
    "  c. adding additional risk splitters to existing rules to improve either accuracy or action rate\n",
    "\n",
    "\n",
    "This script goes over the first use-case, creating a new rule from scratch given a risk-indicator.\n",
    "\n",
    "\n",
    "    I. Set Up (imports, connection to snowflake)\n",
    "    II. Creating the feature driver table \n",
    "    III. Prep the data for the decision tree algorithim\n",
    "    IV. Preliminary analysis on Decision Tree Results\n",
    "    V. Create driver tables to calculate rule KPI\n",
    "    VI. Rule KPI assessment (Decline rate, coverage, accuracy)\n",
    "    VII. Optional modify rule to improve performance, reassessing rule KPI\n",
    "    VIII. Export rule to .py file\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f161b321-7d3e-43e6-b9fd-ddc7432c0981",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Step I: Set Up \n",
    "\n",
    "Importing packages, connecting to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f796a4d-238d-417d-aebe-d97999257082",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install block-cloud-auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1c58315-7a7d-4c0c-83b3-692cf816aea7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandasql==0.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1f520cb-c2d2-41a2-8856-034435d72b70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install sq-pysnowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432c16d9-5eb1-4086-8b73-adac97bc01bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install sqlalchemy-databricks==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9efe8e1f-b2f9-42b8-97f6-ab8a17c778b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e51d7ea9-041f-45ae-86e9-7178da2b082c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_decision_forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2080eb8-0ff8-49d7-9369-74342a831721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install flatten-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7cec5db-a358-48b0-9cae-da5a6c076e32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython() \n",
    "#this is a databricks specific command, it may not be necessary for google notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b813f4-8d89-4a38-b0a3-1beabe2804e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#connect to SF\n",
    "from block_cloud_auth.authenticators import SnowflakeAuthenticator, SnowflakeProvider\n",
    "snowflake_credentials = SnowflakeAuthenticator(SnowflakeProvider()).get_credentials()\n",
    "\n",
    "#provide options\n",
    "options = {\n",
    "    \"sfUrl\": \"https://square.snowflakecomputing.com/\",\n",
    "    \"sfUser\": snowflake_credentials.user,\n",
    "    \"sfPassword\": snowflake_credentials.password,\n",
    "    \"sfDatabase\": snowflake_credentials.database,\n",
    "}\n",
    "\n",
    "#standard inputs and formatting\n",
    "from pysnowflake import Session\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "#pandas sql function\n",
    "run_query = lambda query: sqldf(query, globals())\n",
    "\n",
    "from flatten_dict import flatten\n",
    "\n",
    "#tensor flow imports for decision trees\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6d5a84-6199-4b60-9118-1fb7ce9c0b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#replace below with your ldap name\n",
    "USER_NAME = 'jobyg' \n",
    "\n",
    "#establish snowflake session\n",
    "sess = Session(\n",
    "   connection_override_args={\n",
    "       'autocommit': True,\n",
    "       'authenticator': 'externalbrowser',\n",
    "       'account': 'square',\n",
    "       'database': f'PERSONAL_{USER_NAME.upper()}',\n",
    "       'user': f'{USER_NAME}@squareup.com'\n",
    "   }\n",
    ")\n",
    "#click the url, copy and paste the url and then hit enter to finalize authentication to snowflake \n",
    "conn = sess.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17f5e178-9c19-452a-a49d-e342db03e977",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#use x_large warehouse to speed up querying \n",
    "conn.execute('use warehouse ADHOC__XLARGE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1261743-b0ac-460f-96cb-aab6f7a84de8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 2: Creating the feature driver table\n",
    "\n",
    "\n",
    "Doordash has seen a higher toxicity in recent weeks than normal, and an additional rule is needed to mitigate these losses. To do so, we must first create a driver table which consists of:\n",
    "\n",
    "    1. Identifying columns\n",
    "    2. Features we will use to risk split the new rule\n",
    "    3. Control Group Identification\n",
    "    4. Loss metrics\n",
    "\n",
    "The functions `create_feature_driver` and `create_control_table` will take care of the above:\n",
    "\n",
    "We have to specify some inputs for these functions, namely:\n",
    "\n",
    "    1. the time range of the analysis, \n",
    "    2. the par region\n",
    "    3. checkpoint\n",
    "    4. risk splitting features (for create_feature_driver) \n",
    "    5. the name of the table you will create in snowflake, in the cell below this is the `feature_base_driver_name` variable\n",
    "\n",
    "Note, this function is designed to analyze the **new user** population, if the rule in question should examine tenured users, you will have to modify the underlying function.\n",
    "\n",
    "If you are subsetting the new user population to first orders, or a specific merchant, further filtering must be done with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00ed64dd-ec4a-4aa7-9f2f-8ee2d5319731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functions import create_feature_driver, create_control_table\n",
    "#replace start date and end date for analysis\n",
    "start_date = pd.Timestamp('2024-06-01') #replace with your values\n",
    "end_date = pd.Timestamp('2024-10-05') #replace with your values\n",
    "par_region = 'AU' #replace this\n",
    "checkpoint = 'CHECKOUT_CONFIRM' #replace this \n",
    "feature_base_driver_name='jobyg_fraud_segmentation_tool_notebook_usecase1_demo' #replace this\n",
    "\n",
    "feature_list = ['in_flight_order_merchant_id'\n",
    "             ,'in_flight_order_merchant_name'\n",
    "             ,'in_flight_order_amount'\n",
    "             ,'consumer_contact_address_postcode'\n",
    "             ,'sp_entity_linking_hop0_tot_order_cnt_by_merch_side_email_h72_0'\n",
    "             ,'sp_c_fraud_decline_attempt_d3_0'\n",
    "             ,'sp_c_fraud_decline_attempt_h12_0'\n",
    "             ,'sp_c_fraud_decline_attempt_h1_0'\n",
    "             ,'sp_c_online_ordr_attmpt_credit_card_cnt_h12_0'\n",
    "             ,'in_flight_card_name_vs_profile_name'\n",
    "             ,'sp_c_online_decl_topaz_insffcnt_fund_ordr_cnt_h12_0'\n",
    "             ,'sp_c_online_decl_topaz_insffcnt_fund_ordr_cnt_h168_0'\n",
    "             ,'sp_d_linking_hop0_order_attmpt_cnt_by_device_id_h1_0'\n",
    "             ,'sp_c_pymt_attmpt_cnt_h24_0'\n",
    "             ,'sp_c_order_attempt_cnt_d1'\n",
    "             ,'bp_udp_c_graph_model_score'\n",
    "             ,'inflight_device_id_consumer_distinct_cnt'\n",
    "             ,'sp_address_linking_total_consumer_cnt_by_raw_shipping_hash_d3_0'\n",
    "             ,'bp_c_all_device_linking_cust_cnt'\n",
    "             ,'bp_c_max_device_linking_new_cust_cnt'\n",
    "             ,'consumer_account_linking_type'\n",
    "             ,'bp_c_batch_consumer_batch_model_v1'\n",
    "             ,'model_online_od_payback_non_us_april_2024_score'\n",
    "             ,'consumer_active_order_number'\n",
    "             ,'bp_card_issuing_bank_new_p2d0'\n",
    "             ,'tmx_digital_id_confidence'\n",
    "             ,'bp_profile_email_domain_new_matured_ntl_rate'\n",
    "             ,'derived_minutes_since_account_created'\n",
    "             ,'tmx_smart_learning_fraud_rating'\n",
    "             ,'whitepages_primary_address_checks_is_commercial'\n",
    "             ,'bp_c_acct_cnt_ab_od'\n",
    "             ,'bp_c_seed_based_linking_device_id'\n",
    "             ,'sp_c_order_amt_same_merchant_as_current_h24_0'\n",
    "             ,'sp_c_order_amt_same_merchant_as_current_h12_0'\n",
    "             ,'sp_c_order_amt_same_merchant_as_current_h1_0'\n",
    "             ,'whitepages_identity_check_score'\n",
    "             ,'model_gibberish_consumer_profile_email_august_2022_score'\n",
    "             ,'in_flight_card_name_vs_profile_name'\n",
    "             ,'whitepages_identity_network_score'\n",
    "             ,'sp_c_order_attempt_cnt_d3'\n",
    "             ,'whitepages_primary_email_address_checks_email_first_seen_days'\n",
    "             ,'whitepages_primary_phone_checks_match_to_name'\n",
    "             ,'whitepages_primary_address_checks_match_to_name'\n",
    "             ,'bp_c_outstanding_balance_avg_amt_30d_v2'\n",
    "             ,'bp_c_seed_cnt_linked_by_device_id'\n",
    "             ,'bp_c_seed_cnt_linked_by_raw_shipping_address'\n",
    "             ,'bp_c_trusted_merch_side_email_yn'\n",
    "             ,'consumer_is_first_order'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6512958-4a4e-415b-8ea8-d5a677eef09b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "return_dict = create_feature_driver(feat_list=feature_list, username=USER_NAME, \n",
    "                                    conn=conn, \n",
    "                                    start_date=start_date, \n",
    "                                    end_date=end_date, \n",
    "                                    par_region= par_region,\n",
    "                                    checkpoint=checkpoint, \n",
    "                                    feature_base_driver_name=feature_base_driver_name,\n",
    "                                    skip_assessment = True #speeds up query as I have manually confirmed all the features exist in my authorized view and thus the query won't error out\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa0dff90-7b30-4122-9bde-fb1a8a5c758e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functions import create_control_table\n",
    "control_table_name = 'jobyg_fraud_segmentation_tool_notebook_usecase1_demo_cg'\n",
    "create_control_table(control_table_name, USER_NAME, start_date, end_date, checkpoint, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba14424f-d48e-48ba-b1e8-e1653a6103dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 3: Prepare data for decision tree driver:\n",
    "\n",
    "There are four substeps:\n",
    "    \n",
    "    pulling the data, which we do in the `pull_decision tree_driver` function \n",
    "     \n",
    "    fix datatypes of the feature driver created from the above function (accomplished with the get_datatypes function)\n",
    "\n",
    "    split the data in train and validation datasets using the prep_for_training function\n",
    "\n",
    "    specify decision tree parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5047fdc-b093-4a53-991b-e3e14e12fac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functions import pull_decision_tree_driver\n",
    "\n",
    "dtree_driver = pull_decision_tree_driver(\n",
    "    feat_driver_table=feature_base_driver_name,\n",
    "    control_token_table= control_table_name,\n",
    "    target_column='p2_d0',\n",
    "    conn=conn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5d27024-497f-489b-90d2-749fec4d2cfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtree_driver.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43169d50-4295-4f91-8ce6-be97db0b2151",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Specify Doordash new user online orders in the driver\n",
    "\n",
    "Since this will be a rule designed for Doordash, we must filter the above decision tree driver to just doordash orders, we can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8912ff5-5c65-4f87-9278-7ec752b6c2b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "doordash_driver = run_query('select * from dtree_driver where in_flight_order_merchant_id = \"134317\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3464b143-31e5-4da2-a2cb-dabbf11825a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert datatypes to appropriate values, otherwise we get TypeErrors\n",
    "from functions import get_datatypes #note, if a feature is not behaving as anticipated, most commonly a string input will be treated as an integer, go to the functions script and modify the get_datatypes function to assign the feature it's appropriate type\n",
    "\n",
    "doordash_driver, dtype_dict = get_datatypes(decision_tree_driver=doordash_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c86425b1-7a49-42a0-b898-274b17be839f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "doordash_driver.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b23ddd8a-aca9-4bc5-9c1a-cf3e649101c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functions import prep_for_training\n",
    "\n",
    "\n",
    "#note, i added columns to the exclude feature list if i found the decision tree's splitting\n",
    "#to be too arbitrary and unexplainable and overfitting certain string variables\n",
    "X, doordash_driver   = prep_for_training(decision_tree_driver = doordash_driver,\n",
    "                                      exclude_features=['in_flight_order_merchant_name',\n",
    "                                                        'in_flight_order_merchant_id',\n",
    "                                                        'days_since_first_order_date',\n",
    "                                                        'derived_minutes_since_account_created',\n",
    "                                                        'consumer_contact_address_postcode'],\n",
    "                                      test_ratio=.25) \n",
    "\n",
    "from functions import split_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64fc035b-5515-41db-ab68-a958f9be165c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#split train data\n",
    "\n",
    "train, val = split_dataset(X)\n",
    "\n",
    "#specify column to be used as target\n",
    "label = 'target'\n",
    "#specify target values\n",
    "classes = [0,1]\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train, label=label)\n",
    "val_ds  = tfdf.keras.pd_dataframe_to_tf_dataset(val, label=label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25444ecf-85a8-4f52-814d-51b5e5c80071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#hyperparameter tuning \n",
    "bootstrap_size_ratio = [.25,.5,.75,.85,.9] \n",
    "#bootstrapping ratio determines what percentage of RECORDS is used for training the decision trees,\n",
    "#decision trees that are built on different sets of records, are more likely to be different\n",
    "\n",
    "num_candidate_attributes_ratio = [.4,.55,.7, .8,.85,.925]\n",
    "#num candidate attributes ratio determines what percentage of FEATURES is used for training the decision trees,\n",
    "#decision trees that are built on different sets of features, are more likely to be different\n",
    "\n",
    "train_rows = train.shape[0]\n",
    "min_examples_param_list = [train_rows//100, train_rows//50,train_rows//25, train_rows//10]\n",
    "#min_examples_param_list determines the minimum number of records that must be present when creating segments\n",
    "#for example a min_examples hyper parameter of 100 means no final segment will contain less than 100 records\n",
    "#a higher value of this parameter results in less over-fit segments\n",
    " \n",
    "max_depth = [3,4,5] \n",
    "#max depth determines how many times the tree can split, a higher value of this parameter can result in over-fitting\n",
    "#as the tree will be able to form precise segments that only perform well on the training data through use of excessive splitting\n",
    "\n",
    "num_trees = [100,250,500,600]\n",
    "#num trees determines how many trees are built as part of the random forest, a larger number of trees results in a better performing model, but takes longer to train\n",
    "\n",
    "\n",
    "# Create a Random Search tuner with 50 trials and specified hp configuration.\n",
    "tuner = tfdf.tuner.RandomSearch(num_trials=50)\n",
    "tuner.choice(\"min_examples\", min_examples_param_list)\n",
    "tuner.choice(\"num_candidate_attributes_ratio\", bootstrap_size_ratio)\n",
    "tuner.choice(\"num_trees\", num_trees)\n",
    "tuner.choice(\"max_depth\", max_depth)\n",
    "# tuner.choice(\"allow_na_conditions\", allow_na_conditions)\n",
    "\n",
    "\n",
    "#optional create weights for class labels due to imbalanced nature of p2 d0 transactions\n",
    "total = len(train)\n",
    "pos = np.sum(train['target'] == 1)\n",
    "neg = np.sum(train['target'] == 0)\n",
    " \n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "weight_ratio = weight_for_1/weight_for_0\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    " \n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print(f'ratio of weights is {weight_ratio}')\n",
    "# Define and train the model.\n",
    "tuned_model = tfdf.keras.RandomForestModel(tuner=tuner)\n",
    "tuned_model.fit(train_ds,\n",
    "                #verbose=2 #if you want logs to print, slows down the processing as log printing is slow\n",
    "                class_weight=class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b0922ec-1ed4-453b-bedc-fcd7dd9d9577",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#examine results of the hyper-parameter tuning, notice the difference between the highest and average scores\n",
    "logs = tuned_model.make_inspector().tuning_logs() #assess tuning logs\n",
    "num_trees =logs[logs.best==True]['num_trees'].iloc[0]\n",
    "logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5da155-5756-4762-8d3c-fa31445a0510",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Optional you can specify the first split if you would like by segmenting the dataframe using business intuition. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f77b6009-6707-4d20-a2a4-52b0a31b83a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tfdf.model_plotter.plot_model_in_colab(tuned_model, tree_idx=0, max_depth=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a722170b-05de-4b8d-ba5f-72f2567718d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 5: Preliminary assessment of Decision Tree Results\n",
    "\n",
    "Examine the results of the decision tree. We will look at the control group P2D0 in the time range, as well as a simplified version of the decline volume. We will calculate the true decline rate metrics and coverage in a later step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "822f8d0c-2b59-4b09-b88f-e500a1c05f2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functions import dfs_all_paths\n",
    "inspector = tuned_model.make_inspector()\n",
    "tree = inspector.extract_tree(tree_idx=0)\n",
    "all_paths = []\n",
    "dfs_all_paths(tree.root, [], all_paths)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe106c8c-518f-49db-be30-8dcded8c753c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threshold = .7\n",
    "from functions import get_case_when_statement\n",
    "outputs_dict = get_case_when_statement(tuned_model, threshold=threshold) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b10e6073-fb71-4793-b4b9-604e5c5d81d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_dict = flatten(outputs_dict)\n",
    "for k,v in final_dict.items():\n",
    "    v[0]= v[0].replace('[', '(')\n",
    "    v[0] = v[0].replace(']', ')')\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1691b249-0dda-4391-939c-30c0e5fa2811",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Find high potential segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "020314a2-1e8a-4d85-b74f-c86529fb864c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_rules(df, string_df, rule_dict, threshold=0, target_rule_present=False):\n",
    "    from tqdm import tqdm\n",
    "    from pandasql import sqldf\n",
    "\n",
    "    run_query = lambda query: sqldf(query, globals())\n",
    "    if target_rule_present:\n",
    "        target_rule_ctrl_trxn_ct = df.loc[df['target_rule_flag']==1].target.value_counts().sum()\n",
    "        target_rule_p2_d0 = df.loc[df['target_rule_flag']==1].p2_overdue_d0_local.sum()/df.loc[df['target_rule_flag']==1].p2_due_local.sum()\n",
    "        print(f'target rule ctrl transaction count is {target_rule_ctrl_trxn_ct}, target rule control group p2_d0 is {target_rule_p2_d0}')\n",
    "    performance_dict = {}\n",
    "\n",
    "    query = 'select *'\n",
    "    key_list = list(rule_dict.keys())\n",
    "\n",
    "    first_third = len(key_list)//3\n",
    "    second_third = 2*len(key_list)//3\n",
    "    last_third = len(key_list)\n",
    "\n",
    "    for i in range(0,first_third):\n",
    "        key = key_list[i]\n",
    "        val = rule_dict[key]\n",
    "        query+=f',{val[0]}'\n",
    "            \n",
    "    query += f' from {string_df}'\n",
    "    test_df = run_query(query)\n",
    "    print('evaluating first third of the new rules as sql can only display a limited number of columns')\n",
    "    print('there will be three progress bars as a part of this function, this is progress bar 1')\n",
    "    for i in tqdm(range(0, first_third)):\n",
    "        key = key_list[i]\n",
    "        v = rule_dict[key]\n",
    "        segment_name = v[0].split(' as ')[1]\n",
    "        new_rule_p2_d0 = test_df.loc[test_df[segment_name]==1].p2_overdue_d0_local.sum()/test_df.loc[test_df[segment_name]==1].p2_due_local.sum()\n",
    "        \n",
    "        if target_rule_present:\n",
    "            if new_rule_p2_d0 - target_rule_p2_d0 > threshold:\n",
    "                new_rule_ctrl_trxn_ct = test_df.loc[test_df[segment_name]==1].target.value_counts().sum()\n",
    "                performance_dict[segment_name] = [new_rule_ctrl_trxn_ct,new_rule_p2_d0, v[0]]\n",
    "        elif new_rule_p2_d0 >= .35:\n",
    "            new_rule_ctrl_trxn_ct = test_df.loc[test_df[segment_name]==1].target.value_counts().sum()\n",
    "            performance_dict[segment_name] = [new_rule_ctrl_trxn_ct,new_rule_p2_d0, v[0]]\n",
    "\n",
    "    query = 'select *'\n",
    "    for i in range(first_third, second_third):\n",
    "        key = key_list[i]\n",
    "        val = rule_dict[key]\n",
    "        query+=f',{val[0]}'\n",
    "            \n",
    "    query += f' from {string_df}'\n",
    "    test_df = run_query(query)\n",
    "    print('evaluating second third of the new rules as sql can only display a limited number of columns')\n",
    "    print('there will be three progress bars as a part of this function, this is progress bar 2')\n",
    "    for i in tqdm(range(first_third, second_third)):\n",
    "        key = key_list[i]\n",
    "        v = rule_dict[key]\n",
    "        segment_name = v[0].split(' as ')[1]\n",
    "        new_rule_p2_d0 = test_df.loc[test_df[segment_name]==1].p2_overdue_d0_local.sum()/test_df.loc[test_df[segment_name]==1].p2_due_local.sum()\n",
    "        if target_rule_present:\n",
    "            if new_rule_p2_d0 - target_rule_p2_d0 > threshold:\n",
    "                new_rule_ctrl_trxn_ct = test_df.loc[test_df[segment_name]==1].target.value_counts().sum()\n",
    "                performance_dict[segment_name] = [new_rule_ctrl_trxn_ct,new_rule_p2_d0, v[0]]\n",
    "        elif new_rule_p2_d0 >= .35:\n",
    "            new_rule_ctrl_trxn_ct = test_df.loc[test_df[segment_name]==1].target.value_counts().sum()\n",
    "            performance_dict[segment_name] = [new_rule_ctrl_trxn_ct,new_rule_p2_d0, v[0]]\n",
    "    query = 'select *'\n",
    "    for i in range(second_third, last_third):\n",
    "        key = key_list[i]\n",
    "        val = rule_dict[key]\n",
    "        query+=f',{val[0]}'\n",
    "            \n",
    "    query += f' from {string_df}'\n",
    "    test_df = run_query(query)\n",
    "    print('evaluating last third of the new rules as sql can only display a limited number of columns')\n",
    "    print('there will be three progress bars as a part of this function, this is progress bar 3')\n",
    "    for i in tqdm(range(second_third, last_third)):\n",
    "        key = key_list[i]\n",
    "        v = rule_dict[key]\n",
    "        segment_name = v[0].split(' as ')[1]\n",
    "        new_rule_p2_d0 = test_df.loc[test_df[segment_name]==1].p2_overdue_d0_local.sum()/test_df.loc[test_df[segment_name]==1].p2_due_local.sum()\n",
    "        if target_rule_present:\n",
    "            if new_rule_p2_d0 - target_rule_p2_d0 > threshold:\n",
    "                new_rule_ctrl_trxn_ct = test_df.loc[test_df[segment_name]==1].target.value_counts().sum()\n",
    "                performance_dict[segment_name] = [new_rule_ctrl_trxn_ct,new_rule_p2_d0, v[0]]\n",
    "        elif new_rule_p2_d0 >= .35:\n",
    "            new_rule_ctrl_trxn_ct = test_df.loc[test_df[segment_name]==1].target.value_counts().sum()\n",
    "            performance_dict[segment_name] = [new_rule_ctrl_trxn_ct,new_rule_p2_d0, v[0]]\n",
    "\n",
    "    \n",
    "    return(performance_dict)\n",
    "\n",
    "\n",
    "\n",
    "performance_dict = evaluate_rules(doordash_driver, 'doordash_driver', final_dict, .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "207802fc-a079-4dc4-bf13-9b899475b527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#look at performance of different rules\n",
    "performance_df = pd.DataFrame(performance_dict.values(), columns =['new_rule_ctrl_trxn_ct','new_rule_p2_d0','rule'])\n",
    "performance_df.sort_values(by='new_rule_p2_d0', ascending=False).head(50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1ec41a-dfd0-497a-8e79-88dfc580e7fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Specify a rule from the above top performing segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132810cf-ad60-4e14-82dc-4a2519db0cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rule_to_author = performance_df.iloc[169].rule\n",
    "rule_to_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06c0e2da-b2e5-4ce7-849f-acdba4c2d3f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rule_to_author = performance_df.iloc[261].rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d7cf80-52d8-4c7b-bd43-6ab8c7bafa43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rule_to_author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b401b43c-a16c-49ec-a6fc-39fd7c7437fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### rename segment name to something more user friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae725a96-32c0-4d10-bc1a-68024cea4be2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functions import modify_segment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aa42727-6aff-4706-8491-76e9033f0ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rule_to_author = modify_segment_name(rule_to_author, 'test_segment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f219485-f933-4eb0-8e90-194e2b8c8d81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Step V: Create driver tables for KPI assessment\n",
    "\n",
    "Such as unique declines (# and $), total control group p2 d0, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ab28e4c-6cdf-4e81-84bf-ab47bce84670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "#unique declines\n",
    "unique_rule_decline_table_name = 'jobyg_fraud_segmentation_tool_demo_unique_declines'\n",
    "from functions import create_unique_decline_table\n",
    "create_unique_decline_table(unique_rule_decline_table_name, \n",
    "USER_NAME,\n",
    "start_date,\n",
    "end_date,\n",
    "par_region,\n",
    "checkpoint,\n",
    "conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4030ae8c-f399-42bb-b64d-96dba9f8b10a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functions import get_decline_rate_denom\n",
    "#calaculating the number of tokens and the total amount of gmv attempted\n",
    "decline_rate_denoms = get_decline_rate_denom(start_date, end_date, par_region, checkpoint, USER_NAME, conn)\n",
    "\n",
    "decline_rate_denom_ct = int(decline_rate_denoms.token_ct.values)\n",
    "decline_rate_denom_amt = int(decline_rate_denoms.order_amount.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79b35223-0f98-4921-abe1-a1d4c6540395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calculate the control group loss\n",
    "coverage_denom = dtree_driver.p2_overdue_d0_local.sum() #dtree driver is the inner join between attempt control group and all transaction attempts, summing the overdue column gives us the total new user control group p2_overdue for our respective par region and checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8ecef7-7de1-4753-9d90-4aee1ebcaee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##Create driver table for KPI analysis of the new rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f55d97-ef84-4e09-a51a-aef2ec0e6993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_rule_table_name = 'jobyg_new_fraud_segmentation_tool_notebook_usecase1_demo'\n",
    "grab_new_rule_performance(new_rule_table_name=  new_rule_table_name,\n",
    "                          unique_decline_table_name= unique_rule_decline_table_name,\n",
    "                          rule = rule_to_author,\n",
    "                          start_date=start_date,\n",
    "                          end_date = end_date,\n",
    "                          par_region = par_region,\n",
    "                          checkpoint = checkpoint,\n",
    "                          user_name = USER_NAME,\n",
    "                          conn=conn)\n",
    "\n",
    "#additional query to specify the merchant id for my tablename\n",
    "conn.execute(f'''create or replace table ap_cur_frdrisk_g.public.{new_rule_table_name} as (\n",
    "            select * from ap_cur_frdrisk_g.public.{new_rule_table_name} where in_flight_order_merchant_id = '134317')''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e091f9d-08e8-4902-978e-5ac4d9ce851b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ensure order token has an order amount\n",
    "from functions import order_amount_fixing\n",
    "order_amount_fixing(new_rule_table_name, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de753ea-1d6b-46e0-9a89-56dee70bf851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ensure data is exclusively doordash \n",
    "validation = conn.download(f'''select a.*, coalesce(b.consumer_total_amount_amount, in_flight_order_amount) as order_amount_local from ap_cur_Frdrisk_g.public.{new_rule_table_name}  a\n",
    "                           left join ap_cur_frdrisk_g.public.order_amt_fixed b\n",
    "                           on a.order_token = b.token\n",
    "                           and a.in_flight_order_merchant_id = '134317'\n",
    "                           ''')\n",
    "validation.in_flight_order_merchant_id.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c756f27a-a197-466c-91f7-47d7b4a99415",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b8523df-0457-491f-9f2f-83ea15587861",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Step VI: Calculate KPI of the new rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d0ce4d8-717b-483c-8357-65fe9dbb6f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "analyze_performance('validation','test_segment',decline_rate_denom_ct,decline_rate_denom_amt, coverage_denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4cbcd49-c04c-4239-bc40-a48ff75b07ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step VII: (Optional) Expand upon the initial rule\n",
    "\n",
    "I wanted to see if I can improve the performance of this rule by adding another commonly seen risk splitter, `(sp_c_order_attempt_cnt_d1 >= 3.5)`. The functions created make reassessing the performance of a slightly modified rule -- quick.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a775997-8db8-4a15-b712-1722374b80e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 1: Modify rule logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8565ad50-eec8-4027-a4ce-6cfae2108c9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_rule = 'CASE WHEN (model_online_od_payback_non_us_april_2024_score >= 150.0173797607422) and (in_flight_order_amount >= 13.805000305175781) and (whitepages_identity_check_score >= 302.0) and (whitepages_identity_network_score >= 0.781499981880188) and (sp_c_order_attempt_cnt_d1 >= 3.5) THEN 1 ELSE 0  END as test_segment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ae56957-3080-4e14-9312-1bae7b023afe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2: Create new rule performance driver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "669b9ce1-b491-4c67-b472-19f0e7ff57bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "grab_new_rule_performance(new_rule_table_name=  new_rule_table_name,\n",
    "                          unique_decline_table_name= unique_rule_decline_table_name,\n",
    "                          rule = final_rule,\n",
    "                          start_date=start_date,\n",
    "                          end_date = end_date,\n",
    "                          par_region = par_region,\n",
    "                          checkpoint = checkpoint,\n",
    "                          user_name = USER_NAME,\n",
    "                          conn=conn)\n",
    "\n",
    "#additional query to specify the merchant id for my \n",
    "conn.execute(f'''create or replace table ap_cur_frdrisk_g.public.{new_rule_table_name} as (\n",
    "            select * from ap_cur_frdrisk_g.public.{new_rule_table_name} where in_flight_order_merchant_id = '134317')''')\n",
    "\n",
    "#fix order amount to not have nulls\n",
    "from functions import order_amount_fixing\n",
    "analysis_df = order_amount_fixing(new_rule_table_name, conn)\n",
    "\n",
    "#additional query to specify mercahnt id for my table \n",
    "analysis_df = run_query('''select * from analysis_df where in_flight_order_merchant_id = '134317' ''')\n",
    "analysis_df.in_flight_order_merchant_id.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be511859-4484-44a1-abed-8c30ed68add5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Final Step: Analyze performance of new rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea648ab9-0a12-4eec-be76-6688fc26fa7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "analyze_performance('analysis_df','test_segment',decline_rate_denom_ct,decline_rate_denom_amt, coverage_denom) #worse unique KPI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b33d01e6-b331-4dac-9ab8-5b8d72ed8c01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Continue to refine the rule performance\n",
    "\n",
    "Let's try one additional modification focusing on `sp_c_order_amt_same_merchant_as_current_h24_0`, increasing the threshold from 13.8 to 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "931bf426-1f0d-4773-bbd0-61614a14ff56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_rule2 = 'CASE WHEN (model_online_od_payback_non_us_april_2024_score >= 150.0173797607422) and (in_flight_order_amount >= 13.80) and (whitepages_identity_check_score >= 280.0) and (whitepages_identity_network_score >= 0.761499981880188) and (sp_c_order_amt_same_merchant_as_current_h24_0 >= 13.575) THEN 1 ELSE 0  END as test_segment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78489957-cbba-4ad3-a169-9e1dd6d3d185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grab_new_rule_performance(new_rule_table_name =  new_rule_table_name,\n",
    "                          unique_decline_table_name= unique_rule_decline_table_name,\n",
    "                          rule = final_rule2,\n",
    "                          start_date=start_date,\n",
    "                          end_date = end_date,\n",
    "                          par_region = par_region,\n",
    "                          checkpoint = checkpoint,\n",
    "                          user_name = USER_NAME,\n",
    "                          conn=conn)\n",
    "\n",
    "#additional query to specify the merchant id for my \n",
    "conn.execute(f'''create or replace table ap_cur_frdrisk_g.public.{new_rule_table_name} as (\n",
    "            select * from ap_cur_frdrisk_g.public.{new_rule_table_name} where in_flight_order_merchant_id = '134317')''')\n",
    "\n",
    "#fix order amount to not have nulls\n",
    "from functions import order_amount_fixing\n",
    "analysis_df = order_amount_fixing(new_rule_table_name, conn)\n",
    "\n",
    "#additional query to specify mercahnt id for my table \n",
    "analysis_df = run_query('''select * from analysis_df where in_flight_order_merchant_id = '134317' ''')\n",
    "analysis_df.in_flight_order_merchant_id.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a077607-5f92-4eaf-9120-bd1e4d062a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "analyze_performance('analysis_df','test_segment',decline_rate_denom_ct,decline_rate_denom_amt, coverage_denom) #rule performance looks promising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "776180e2-fa87-4bd1-8469-ee2f0a3858e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step VIII: Export rule to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59c97767-bdb9-4fd8-997a-455b357a8b23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from functions import create_py_rule\n",
    "final_rule2 = create_py_rule(final_rule2,  rule_name='AU_Online_Doordash_rule.py', path_name ='/Workspace/Users/jobyg@squareup.com/Fraud Segmentation Tool /',debug=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fraud Segmentation Tool Use Case 1: New Rule Development",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
